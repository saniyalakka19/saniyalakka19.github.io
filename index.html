<!DOCTYPE html>
<html>
<head>
<title>Neural Information Retrieval</title>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
<style>
body {font-family: "Times New Roman", Georgia, Serif;}
h1, h2, h3, h4, h5, h6 {
  font-family: "Playfair Display";
  letter-spacing: 5px;
}
.test{
    padding: 20px 25px;
    border-radius: 5px;
    background-color: #b8d2fa;
}

.column {
  float: left;
  width: 33.33%;
}

/* Clear floats after the columns */
.row:after {
  content: "";
  display: table;
  clear: both;
}


.button {
  display: inline-block;
  padding: 15px 25px;
  font-size: 24px;
  cursor: pointer;
  text-align: center;
  text-decoration: none;
  outline: none;
  color: black;
  background-color: #b8d2fa;
  border: none;
  border-radius: 15px;
  transition-duration: 0.4s;
}

.button:hover {background-color: #75a7f5}

.text-center {
  text-align: center;
}
.head{
    color: white;
}

</style>
</head>
<body>

<!-- Navbar (sit on top) -->
<div class="w3-top">
  <div class="w3-bar w3-white w3-padding w3-card" style="letter-spacing:4px;">
    <a href="#home" class="w3-bar-item w3-button">University of California Berkeley Capstone</a>
    <!-- Right-sided navbar links. Hide them on small screens -->
    <div class="w3-right w3-hide-small">
      <a href="#team" class="w3-bar-item w3-button">Team</a>
      <a href="#about" class="w3-bar-item w3-button">About</a>
      <a href="#approach" class="w3-bar-item w3-button">Approach</a>
      <a href="#data" class="w3-bar-item w3-button">Data</a>
      <a href="#results" class="w3-bar-item w3-button">Results</a>
    </div>
  </div>
</div>

<!-- Header -->
<header class="w3-display-container w3-content w3-wide" style="max-width:1600px;min-width:500px" id="home">
  <img class="w3-image" src="header.jpg" alt="Hamburger Catering" width="1600" height="800">
  <div class="w3-display-bottomleft w3-padding-large w3-opacity">
    <h2 class="head"> With Splade</h2>
  </div>
</header>

<!-- Page content -->
<div class="w3-content" style="max-width:1100px">

  <!-- Team Section -->
  <div class="w3-row w3-padding-64" id="team">
    </div>
      <h1 class="w3-center">The Team</h1><br>
      <div class="row" id="test1">
        <div class="column">
            <img class="text-center" src="saniya.jpg" alt="Hamburger Catering" width="400" height="400">
            <h4 class="text-center"><a href="https://www.linkedin.com/in/saniya-lakka-863289160/" class="button">Saniya Lakka</a></h4>
        </div>
        <div class="column">
            <img class="text-center" src="nicholas.jpg" alt="Hamburger Catering" width="400" height="400">
            <h4 class="text-center"><a href="https://www.linkedin.com/in/nicholas-schantz/" class="button">Nicholas Schantz</a></h4>
        </div>
        <div class="column">
            <img class="text-center" src="yi.jpg" alt="Hamburger Catering" width="400" height="400">
            <h4 class="text-center"><a href="https://www.linkedin.com/in/yizhang7210/" class="button">Yi Zhang</a></h4>
        </div>
      </div>
    </div>
  </div>

  <br>
  <br>
  <br>
  <br>
  <hr>

  <!-- About Section -->
  <div class="w3-row w3-padding-64" id="about">
    <div class="w3-col m6 w3-padding-large w3-hide-small">
        <br>
        <br>
        <br>
     <img src="splade.jpg" class="w3-round w3-image w3-opacity-min" alt="Table Setting" width="600" height="750">
    </div>

    <div class="w3-col m6 w3-padding-large">
    <div class="test">
      <h1 class="w3-center">About</h1><br>
      <h5 class="w3-center">What is Neural Information Retrieval?</h5>
      <p class="w3-large">Information retrieval is a tool that almost everyone is familiar with. If you have ever used a search engine (Google, Bing, Amazon, Online Store search etc.) then you have used information retrieval. Google’s Search statistics show that there are over 8.5 billion searches over a day. Search engines play a large role in everyone’s day to day life, whether it is for school, work, or need of a quick answer, we are constantly online making search requests. Any incremental improvements in the search algorithm can have  a significant impact. Our goal is to improve search engines by decreasing the time in the retrieval process. To conquer this task, we will be leveraging the algorithm SPLADE.  </p>
      <h5 class="w3-center">What is SPLADE?</h5>
      <p class="w3-large">SPLADE stands for Sparse Lexical AnD Expansion Model for first stage ranking. A common issue with search engines is the inability to pick up passages unless it contains the exact vocabulary in the search query. SPLADE attempts to solve this issue by using query expansion in order to acquire more accurate passages. This is done by using the BERT MLM head and sparse regularization.</p>
    </div>
    </div>
  </div>
  
  <hr>
  
  <!-- Approach Section -->
  <div class="w3-row w3-padding-64" id="approach">
    <div class="w3-col l6 w3-padding-large">
        <div class="test">
      <h1 class="w3-center">Our Approach</h1><br>
      <p>In the original SPLADE information retrieval (IR) system, the SPLADE generated query and document embedding similarity scores are exhaustively calculated. That is for Q queries and D documents, we generate QxD similarities. Our approaches center around improving the search component of the IR system by reducing the number of Query-Document similarity calculations computed at inference time.
        </p>
    <h4>Topic-Based Approach</h4>
    <p>The high level idea here is to apply a faster filter to passages during document retrieval based on its topic, before it gets ranked along with all the passages. This insight comes from the observation that typically a query only falls within the realm of a small number of topics (e.g. sports, or history), and as a result, there are a large number of passages that can be filtered out purely based on its topic and does not need to enter the ranking stage of retrieval, since even if their score is 0, it still contributes to the time complexity of the ranking algorithm. As a result, our hypothesis is that it will make retrieval faster, especially when there are a large number of passages to retrieve from.
        <br>
        Concretely, our retrieval algorithm is as follows:
        <br>
        <ul>
            <li>In addition to having a sparse index in the vocabulary space (BOW) for each passage, also use a pre-trained topic model to generate a sparse index which contains a topic score for each passage over the fixed topics (there are 19 of them).</li>
            <li>Given an incoming query, use the same model to score this query against the same topics</li>
            <li>Trim the topic vector of this query so only scores above a QUERY_THRESHOLD remain, and other topics will be set to 0. This makes the topic vector sparse.</li>
            <li>Load the topic scores of all passages in memory, then trim them as well, so only scores above PASSAGE_THRESHOLD remain, the other entries will be set to 0. This makes the passage topic vectors sparse too.</li>
            <li>As part of the SPLADE process, when scoring the passages for each query, consider if the query has any overlap of topics with the passage. If not, directly eliminate this passage from consideration and exclude this passage from the ranking stage. If yes, multiply the SPLADE score by the dot product (or cosine similarity) between the topic vectors of the query and the passage. As a result, passages that have a larger overlap with the query will get a higher score.</li>
        </ul>
        </p>

        <h4>Binary Tree Approach</h4>
    <p>  We have the SPLADE generated weights for each term in the BERT embedding space (about 35,000 terms) for each document. So for each document we have a 35000 length 1-D vector.
        With no pre-sorting, we can recursively split the documents into groups, effectively making a binary tree. At each node, we take all documents in the subtree and calculate an aggregate metric for each term in the embedding space. We found better results using the max than using the average of the SPLADE generated term weights.
        <br>
        <br>
        At inference time, when a query is supplied, rather than compared against all available documents, we calculate the similarity of the query to the aggregate document embedding at each node in the tree. We then follow the path of the tree with the higher similarity score. We can parameterize the number of documents returned by setting a predetermined depth of the tree or counting the number of leaf nodes available in the subtree.
        <br>
        <br>
        We find this approach to have compelling results based on preliminary MRR@10 scores and the potential to greatly improve the retrieval efficiency of the IR system.
        <br>
        <br>
        We are actively developing this approach, researching pre-sorting of the documents by similarity through hierarchical clustering and topic classification.
        </p>
    </div>
    </div>
    
    <div class="w3-col l6 w3-padding-large">
        <img src="topic.jpg" class="w3-round w3-image"  alt="Table Setting" width="700" height="770">
        <br>
        <br>
        <img src="binary.jpg" class="w3-round w3-image"  alt="Table Setting" width="700" height="770">
    </div>
  </div>


  <!-- Data Section -->
  <div class="w3-row w3-padding-64" id="data">
    <div class="w3-col m6 w3-padding-large w3-hide-small">
        <br>
        <br>
        <br>

    <img src="passage.jpg" class="w3-round w3-image" alt="Table Setting" width="600" height="750">
    <h4 class="text-center"><a href="https://microsoft.github.io/msmarco/Datasets.html" class="button">MSMARCO Dataset</a></h4>
    </div>

    <div class="w3-col m6 w3-padding-large">
        <div class="test">
      <h1 class="w3-center">The Data</h1><br>
      <h5 class="w3-center">MSMARCO</h5>
      <p class="w3-large">For our project we leveraged the MS MARCO dataset consisting of 1,010,916 anonymised questions sampled from Bing’s search query logs as well as 8,841,823 passages extracted from 353,535 web documents. A query is a string that is inputted into the search bar. Through EDA we discovered that the number of words in a query ranges from 5 to 12 and mostly falls between 5 to 7 words. The passage is ideally a relevant result that is outputted after the query is inputted. From EDA we found that passages range from 0 to 1000 characters and are generally between ~230 to ~260 characters.</p>
    </div>
    </div>
  </div>

  <hr>

  <!-- Results Section -->
  <div class="w3-row w3-padding-64" id="results">
    </div>
      <h1 class="w3-center">Results</h1><br>
    </div>
   </div>
   

  <hr>
  
<!-- End page content -->
</div>

<!-- Footer -->
<footer class="w3-center w3-light-grey w3-padding-32">
  <p>Last Updated 03/13/2023</p>
</footer>

</body>
</html>
